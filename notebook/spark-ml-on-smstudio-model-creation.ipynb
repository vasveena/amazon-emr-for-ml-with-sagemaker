{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-FH2OVSN51AM0) details\n",
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1647475657060_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-20-36.us-east-2.compute.internal:20888/proxy/application_1647475657060_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-20-151.us-east-2.compute.internal:8042/node/containerlogs/container_1647475657060_0008_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-FH2OVSN51AM0\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --cluster-id j-FH2OVSN51AM0 --auth-type None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SparkML dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    VectorIndexer,\n",
    "    OneHotEncoderEstimator,\n",
    "    VectorAssembler,\n",
    "    IndexToString,\n",
    ")\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"sex\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"diameter\", DoubleType(), True),\n",
    "        StructField(\"height\", DoubleType(), True),\n",
    "        StructField(\"whole_weight\", DoubleType(), True),\n",
    "        StructField(\"shucked_weight\", DoubleType(), True),\n",
    "        StructField(\"viscera_weight\", DoubleType(), True),\n",
    "        StructField(\"shell_weight\", DoubleType(), True),\n",
    "        StructField(\"rings\", DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "|sex|length|diameter|height|whole_weight|shucked_weight|viscera_weight|shell_weight|rings|\n",
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "|  M| 0.455|   0.365| 0.095|       0.514|        0.2245|         0.101|        0.15| 15.0|\n",
      "|  M|  0.35|   0.265|  0.09|      0.2255|        0.0995|        0.0485|        0.07|  7.0|\n",
      "|  F|  0.53|    0.42| 0.135|       0.677|        0.2565|        0.1415|        0.21|  9.0|\n",
      "|  M|  0.44|   0.365| 0.125|       0.516|        0.2155|         0.114|       0.155| 10.0|\n",
      "|  I|  0.33|   0.255|  0.08|       0.205|        0.0895|        0.0395|       0.055|  7.0|\n",
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "total_df = spark.read.csv(\"s3://vasveena-test-demo/emr-sagemaker-ml/smstudio/inputdata/abalone.csv\", header=False, schema=schema)\n",
    "total_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split input data into train and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_df, validation_df) = total_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abalone dataset has one categorical column - \"sex\" which needs to be converted to integer format before it can be passed to the Random Forest algorithm.\n",
    "\n",
    "For that, we are using StringIndexer and OneHotEncoder from Spark to transform the categorical column and then use a VectorAssembler to produce a flat one dimensional vector for each data-point so that it can be used with the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "\n",
    "sex_encoder = OneHotEncoderEstimator(inputCols=[\"indexed_sex\"], outputCols=[\"sex_vec\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"sex_vec\",\n",
    "        \"length\",\n",
    "        \"diameter\",\n",
    "        \"height\",\n",
    "        \"whole_weight\",\n",
    "        \"shucked_weight\",\n",
    "        \"viscera_weight\",\n",
    "        \"shell_weight\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Random Forest model and perform training\n",
    "After the data is preprocessed, we define a RandomForestClassifier, define our Pipeline comprising of both feature transformation and training stages and train the Pipeline calling .fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"rings\", featuresCol=\"features\", maxDepth=6, numTrees=18)\n",
    "\n",
    "pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained Model to transform train and validation dataset\n",
    "\n",
    "Next we will use this trained Model to convert our training and validation dataset to see some sample output and also measure the performance scores.The Model will apply the feature transformers on the data before passing it to the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       prediction|\n",
      "+-----------------+\n",
      "|7.111950248271179|\n",
      "|6.278434008717797|\n",
      "|7.684682253679232|\n",
      "|7.571505070167983|\n",
      "|7.477313150976063|\n",
      "+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "transformed_train_df = model.transform(train_df)\n",
    "\n",
    "transformed_validation_df = model.transform(validation_df)\n",
    "\n",
    "transformed_validation_df.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on train and validation dataset\n",
    "\n",
    "Using Spark’s RegressionEvaluator, we can calculate the rmse (Root-Mean-Squared-Error) on our train and validation dataset to evaluate its performance. If the performance numbers are not satisfactory, we can train the model again and again by changing parameters of Random Forest or add/remove feature transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 2.06362\n",
      "Validation RMSE = 2.31196"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"rings\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "train_rmse = evaluator.evaluate(transformed_train_df)\n",
    "validation_rmse = evaluator.evaluate(transformed_validation_df)\n",
    "print(\"Train RMSE = %g\" % train_rmse)\n",
    "print(\"Validation RMSE = %g\" % validation_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLeap to serialize the model\n",
    "\n",
    "By calling the serializeToBundle method from the MLeap library, we can store the Model in a specific serialization format that can be later used for inference by sagemaker-sparkml-serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.serializeToBundle(\"jar:file:/tmp/model.zip\", transformed_validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model to tar.gz format\n",
    "SageMaker expects any model format to be present in tar.gz format, but MLeap produces the model zip format. In the next cell, we unzip the model artifacts and store it in tar.gz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"/tmp/model.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/model\")\n",
    "\n",
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"/tmp/model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/model/bundle.json\", arcname=\"bundle.json\")\n",
    "    tar.add(\"/tmp/model/root\", arcname=\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the trained model artifacts to S3\n",
    "At the end, we need to upload the trained and serialized model artifacts to S3 so that it can be used for inference in SageMaker.\n",
    "\n",
    "Please note down the S3 location to where you are uploading your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "file_name = os.path.join(\"emr/abalone/mleap\", \"model.tar.gz\")\n",
    "s3.Bucket(\"sagemaker-us-east-2-620614497509\").upload_file(\"/tmp/model.tar.gz\", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model artifacts from local disk (optional)\n",
    "If you are training multiple ML models on the same host and using the same location to save the MLeap serialized model, then you need to delete the model on the local disk to prevent MLeap library failing with an error - file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.remove(\"/tmp/model.zip\")\n",
    "os.remove(\"/tmp/model.tar.gz\")\n",
    "shutil.rmtree(\"/tmp/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting the model in SageMaker\n",
    "Now the second phase of this Notebook begins, where we will host this model in SageMaker and perform predictions against it.\n",
    "\n",
    "For this, we are going to switch to Python3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting a model in SageMaker requires two components\n",
    "\n",
    "A Docker image residing in ECR.\n",
    "A trained Model residing in S3.\n",
    "\n",
    "For SparkML, Docker image for MLeap based SparkML serving has already been prepared and uploaded to ECR by SageMaker team which anyone can use for hosting. For more information on this, please see SageMaker SparkML Serving.\n",
    "\n",
    "MLeap serialized model was uploaded to S3 as part of the Spark job we executed in EMR in the previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the endpoint for prediction\n",
    "Next we’ll create the SageMaker endpoint which will be used for performing online prediction.\n",
    "\n",
    "For this, we have to create an instance of SparkMLModel from sagemaker-python-sdk which will take the location of the model artifacts that we uploaded to S3 as part of the EMR job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the schema of the payload via environment variable\n",
    "\n",
    "SparkML server also needs to know the payload of the request that’ll be passed to it while calling the predict method. In order to alleviate the pain of not having to pass the schema with every request, sagemaker-sparkml-serving lets you to pass it via an environment variable while creating the model definitions.\n",
    "\n",
    "We will see later that you can overwrite this schema on a per request basis by passing it as part of the individual request payload as well.\n",
    "\n",
    "This schema definition should also be passed while creating the instance of SparkMLModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input\": [\n",
      "    {\n",
      "      \"name\": \"sex\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"length\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"diameter\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"height\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"whole_weight\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"shucked_weight\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"viscera_weight\",\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"shell_weight\",\n",
      "      \"type\": \"double\"\n",
      "    }\n",
      "  ],\n",
      "  \"output\": {\n",
      "    \"name\": \"prediction\",\n",
      "    \"type\": \"double\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\"name\": \"sex\", \"type\": \"string\"},\n",
    "        {\"name\": \"length\", \"type\": \"double\"},\n",
    "        {\"name\": \"diameter\", \"type\": \"double\"},\n",
    "        {\"name\": \"height\", \"type\": \"double\"},\n",
    "        {\"name\": \"whole_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"shucked_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"viscera_weight\", \"type\": \"double\"},\n",
    "        {\"name\": \"shell_weight\", \"type\": \"double\"},\n",
    "    ],\n",
    "    \"output\": {\"name\": \"prediction\", \"type\": \"double\"},\n",
    "}\n",
    "schema_json = json.dumps(schema, indent=2)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Initialize sagemaker session\n",
    "session = sagemaker.Session(\n",
    "    boto_session=boto3_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=sagemaker_runtime_client,\n",
    ")\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 location of where you uploaded your trained and serialized SparkML model\n",
    "sparkml_data = \"s3://{}/{}/{}\".format(\n",
    "    \"sagemaker-us-east-2-620614497509\", \"emr/abalone/mleap\", \"model.tar.gz\"\n",
    ")\n",
    "model_name = \"sparkml-abalone-\" + timestamp_prefix\n",
    "sparkml_model = SparkMLModel(\n",
    "    model_data=sparkml_data,\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    name=model_name,\n",
    "    # passing the schema defined above by using an environment\n",
    "    # variable that sagemaker-sparkml-serving understands\n",
    "    env={\"SAGEMAKER_SPARKML_SCHEMA\": schema_json},\n",
    ")\n",
    "\n",
    "\n",
    "endpoint_name = \"sparkml-abalone-ep-\" + timestamp_prefix\n",
    "sparkml_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.c4.xlarge\", endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the newly created inference endpoint with a payload to transform the data\n",
    "Now we will invoke the endpoint with a valid payload that sagemaker-sparkml-serving can recognize. There are three ways in which input payload can be passed to the request:\n",
    "\n",
    "Pass it as a valid CSV string. In this case, the schema passed via the environment variable will be used to determine the schema. For CSV format, every column in the input has to be a basic datatype (e.g. int, double, string) and it can not be a Spark Array or Vector.\n",
    "\n",
    "Pass it as a valid JSON string. In this case as well, the schema passed via the environment variable will be used to infer the schema. With JSON format, every column in the input can be a basic datatype or a Spark Vector or Array provided that the corresponding entry in the schema mentions the correct value.\n",
    "\n",
    "Pass the request in JSON format along with the schema and the data. In this case, the schema passed in the payload will take precedence over the one passed via the environment variable (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the payload in CSV format\n",
    "We will first see how the payload can be passed to the endpoint in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer, JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "payload = \"F,0.515,0.425,0.14,0.766,0.304,0.1725,0.255\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=session, serializer=CSVSerializer()\n",
    ")\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the payload in JSON format\n",
    "We will now pass a different payload in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"data\": [\"F\", 0.515, 0.425, 0.14, 0.766, 0.304, 0.1725, 0.255]}\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=session, serializer=JSONSerializer()\n",
    ")\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the payload with both schema and the data\n",
    "Next we will pass the input payload comprising of both the schema and the data. If you notice carefully, this schema will be slightly different than what we have passed via the environment variable. The locations of length and sex column have been swapped and so the data. The server now parses the payload with this schema and works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "            {\"name\": \"length\", \"type\": \"double\"},\n",
    "            {\"name\": \"sex\", \"type\": \"string\"},\n",
    "            {\"name\": \"diameter\", \"type\": \"double\"},\n",
    "            {\"name\": \"height\", \"type\": \"double\"},\n",
    "            {\"name\": \"whole_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"shucked_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"viscera_weight\", \"type\": \"double\"},\n",
    "            {\"name\": \"shell_weight\", \"type\": \"double\"},\n",
    "        ],\n",
    "        \"output\": {\"name\": \"prediction\", \"type\": \"double\"},\n",
    "    },\n",
    "    \"data\": [0.515, \"F\", 0.425, 0.14, 0.766, 0.304, 0.1725, 0.255],\n",
    "}\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=session, serializer=JSONSerializer()\n",
    ")\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the Endpoint (Optional)\n",
    "Next we will delete the endpoint so that you do not incur the cost of keeping it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
